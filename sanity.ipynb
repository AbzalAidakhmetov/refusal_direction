{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62c0184",
   "metadata": {},
   "source": [
    "## Replication of Refusal Direction Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e36c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Part 0: Setup - Imports, Model, Tokenizer, and Data\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import nnsight\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Seeding for reproducibility ---\n",
    "# This must be done before any model loading or operations.\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # # Set environment variable for deterministic CUDA operations\n",
    "    # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "# Configure PyTorch for deterministic behavior\n",
    "# Note: This can impact performance.\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Add project root to path to import dataset loader\n",
    "# Make sure this path is correct for your environment\n",
    "sys.path.append('.')\n",
    "from dataset.load_dataset import load_dataset_split\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = 'google/gemma-2b-it'\n",
    "N_TRAIN_SAMPLES = 128 # Keep low for faster execution\n",
    "N_VAL_SAMPLES = 32   # Keep low for faster execution\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28a761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Model and Tokenizer ---\n",
    "# We load in 4bit for memory efficiency. nnsight will handle it.\n",
    "model = LanguageModel(\n",
    "    MODEL_PATH,\n",
    "    device_map=DEVICE,\n",
    "    torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7870041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Datasets ---\n",
    "harmful_train = random.sample(load_dataset_split(harmtype='harmful', split='train', instructions_only=True), N_TRAIN_SAMPLES)\n",
    "harmless_train = random.sample(load_dataset_split(harmtype='harmless', split='train', instructions_only=True), N_TRAIN_SAMPLES)\n",
    "\n",
    "val_harmful_all = load_dataset_split(harmtype='harmful', split='val', instructions_only=True)\n",
    "val_harmless_all = load_dataset_split(harmtype='harmless', split='val', instructions_only=True)\n",
    "N_VAL_SAMPLES = min(N_VAL_SAMPLES, len(val_harmful_all), len(val_harmless_all))\n",
    "harmful_val = random.sample(val_harmful_all, N_VAL_SAMPLES)\n",
    "harmless_val = random.sample(val_harmless_all, N_VAL_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3bc64f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 post-instruction tokens.\n",
      "Post-instruction string: '<end_of_turn>\\n<start_of_turn>model\\n'\n",
      "Post-instruction tokens: [107, 108, 106, 2516, 108]\n",
      "Will extract activations from the last 5 token positions: [-5, -4, -3, -2, -1]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Part 1: Post-Instruction Tokens Calculated\n",
    "# ==============================================================================\n",
    "GEMMA_CHAT_TEMPLATE = \"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "def format_instruction(instruction):\n",
    "    return GEMMA_CHAT_TEMPLATE.format(instruction=instruction)\n",
    "\n",
    "# Find the end-of-instruction (eoi) tokens\n",
    "eoi_string = GEMMA_CHAT_TEMPLATE.split(\"{instruction}\")[-1]\n",
    "eoi_toks = tokenizer.encode(eoi_string, add_special_tokens=False)\n",
    "TOKEN_POSITIONS = list(range(-len(eoi_toks), 0))\n",
    "\n",
    "print(f\"Found {len(eoi_toks)} post-instruction tokens.\")\n",
    "print(f\"Post-instruction string: {repr(eoi_string)}\")\n",
    "print(f\"Post-instruction tokens: {eoi_toks}\")\n",
    "print(f\"Will extract activations from the last {len(TOKEN_POSITIONS)} token positions: {TOKEN_POSITIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4fc8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated refusal token IDs: [235285, 590]\n",
      "Corresponding decoded tokens: 'I', ' I'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Part 3: Replicating Evaluation Metrics from the Project\n",
    "# ==============================================================================\n",
    "# These functions are adapted directly from `pipeline/submodules/select_direction.py`\n",
    "\n",
    "REFUSAL_PHRASES = [\"I\",\n",
    "    # \"I'm sorry\", \"I am sorry\", \"I apologize\", \"As an AI\", \n",
    "    # \"As a language model\", \"As an assistant\", \"I cannot\", \"I can't\", \n",
    "    # \"I am unable to\", \"I'm unable to\", \"I am not able to\", \"I'm not able to\"\n",
    "]\n",
    "\n",
    "# Get the first token ID for each refusal phrase.\n",
    "# We use a set to automatically handle duplicate token IDs (e.g., many phrases start with \"I\").\n",
    "refusal_token_ids = set()\n",
    "for phrase in REFUSAL_PHRASES:\n",
    "    token_ids = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "    if token_ids:\n",
    "        refusal_token_ids.add(token_ids[0])\n",
    "\n",
    "# The Gemma tokenizer might also produce different tokens for words with a leading\n",
    "# space, which is common at the start of a generation. Let's add those too.\n",
    "for phrase in REFUSAL_PHRASES:\n",
    "    token_ids_with_space = tokenizer.encode(\" \" + phrase, add_special_tokens=False)\n",
    "    if token_ids_with_space:\n",
    "        # If the first token is just a space, we want the *next* token.\n",
    "        if tokenizer.decode(token_ids_with_space[0]).strip() == \"\" and len(token_ids_with_space) > 1:\n",
    "            refusal_token_ids.add(token_ids_with_space[1])\n",
    "        else:\n",
    "            refusal_token_ids.add(token_ids_with_space[0])\n",
    "\n",
    "# Convert the set to a list for indexing and update the global variable.\n",
    "GEMMA_REFUSAL_TOKS = list(refusal_token_ids)\n",
    "\n",
    "print(f\"Updated refusal token IDs: {GEMMA_REFUSAL_TOKS}\")\n",
    "decoded_tokens = [f\"'{tokenizer.decode([tok])}'\" for tok in GEMMA_REFUSAL_TOKS]\n",
    "print(f\"Corresponding decoded tokens: {', '.join(decoded_tokens)}\")\n",
    "\n",
    "def refusal_score_fn(logits, refusal_toks=GEMMA_REFUSAL_TOKS, epsilon=1e-8):\n",
    "    logits = logits.to(torch.float64)[:, -1, :] # Last token position\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    refusal_probs = probs[:, refusal_toks].sum(dim=-1)\n",
    "    nonrefusal_probs = 1.0 - refusal_probs\n",
    "    return torch.log(refusal_probs + epsilon) - torch.log(nonrefusal_probs + epsilon)\n",
    "\n",
    "def kl_div_fn(logits_a, logits_b, epsilon=1e-6):\n",
    "    probs_a = logits_a.to(torch.float64).softmax(dim=-1)\n",
    "    probs_b = logits_b.to(torch.float64).softmax(dim=-1)\n",
    "    kl_divs = (probs_a * (torch.log(probs_a + epsilon) - torch.log(probs_b + epsilon))).sum(dim=-1)\n",
    "    return kl_divs.mean()\n",
    "\n",
    "def filter_fn(refusal_score, steering_score, kl_div_score, layer, n_layer, kl_threshold=0.7, induce_refusal_threshold=0.5, prune_layer_percentage=0.2):\n",
    "    if np.isnan(refusal_score) or np.isnan(steering_score) or np.isnan(kl_div_score):\n",
    "        return True\n",
    "    if prune_layer_percentage is not None and layer >= int(n_layer * (1.0 - prune_layer_percentage)):\n",
    "        return True\n",
    "    if kl_threshold is not None and kl_div_score > kl_threshold:\n",
    "        return True\n",
    "    if induce_refusal_threshold is not None and steering_score < induce_refusal_threshold:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9357fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional, it only removes a couple of examples, but it's a good idea to filter the datasets to only include\n",
    "# examples that the model strongly refuses or complies with by default. \n",
    "\n",
    "# # ==============================================================================\n",
    "# # Part 1.5: Filter Datasets for a Cleaner Signal\n",
    "# # ==============================================================================\n",
    "# # As in the project's pipeline, we filter our datasets to only include\n",
    "# # examples that the model strongly refuses or complies with by default.\n",
    "\n",
    "# def get_refusal_scores_for_dataset(instructions):\n",
    "#     \"\"\"Helper to get refusal scores for a list of instructions.\"\"\"\n",
    "#     scores = []\n",
    "#     # Process in batches to avoid OOM errors\n",
    "#     batch_size = 4\n",
    "#     for i in tqdm(range(0, len(instructions), batch_size), desc=\"Getting refusal scores\"):\n",
    "#         batch = instructions[i:i+batch_size]\n",
    "#         formatted_batch = [format_instruction(p) for p in batch]\n",
    "#         with model.trace(formatted_batch, scan=False, validate=False):\n",
    "#             logits_proxy = model.output.logits.save()\n",
    "#         # Using refusal_score_fn from Part 3\n",
    "#         batch_scores = refusal_score_fn(logits_proxy).cpu().detach().numpy().tolist()\n",
    "#         scores.extend(batch_scores)\n",
    "#     return scores\n",
    "\n",
    "# # Get scores for all datasets\n",
    "# harmful_train_scores = get_refusal_scores_for_dataset(harmful_train)\n",
    "# harmless_train_scores = get_refusal_scores_for_dataset(harmless_train)\n",
    "# harmful_val_scores = get_refusal_scores_for_dataset(harmful_val)\n",
    "# harmless_val_scores = get_refusal_scores_for_dataset(harmless_val)\n",
    "\n",
    "# # Filter the datasets\n",
    "# harmful_train_filtered = [p for p, s in zip(harmful_train, harmful_train_scores) if s > 0]\n",
    "# harmless_train_filtered = [p for p, s in zip(harmless_train, harmless_train_scores) if s < 0]\n",
    "# harmful_val_filtered = [p for p, s in zip(harmful_val, harmful_val_scores) if s > 0]\n",
    "# harmless_val_filtered = [p for p, s in zip(harmless_val, harmless_val_scores) if s < 0]\n",
    "\n",
    "# print(\"--- Dataset Filtering Results ---\")\n",
    "# print(f\"Harmful Train:   {len(harmful_train_filtered)} / {len(harmful_train)} kept\")\n",
    "# print(f\"Harmless Train:  {len(harmless_train_filtered)} / {len(harmless_train)} kept\")\n",
    "# print(f\"Harmful Val:     {len(harmful_val_filtered)} / {len(harmful_val)} kept\")\n",
    "# print(f\"Harmless Val:    {len(harmless_val_filtered)} / {len(harmless_val)} kept\")\n",
    "\n",
    "# # Overwrite the original variables with the filtered ones for the rest of the notebook\n",
    "# harmful_train = harmful_train_filtered\n",
    "# harmless_train = harmless_train_filtered\n",
    "# harmful_val = harmful_val_filtered\n",
    "# harmless_val = harmless_val_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934fa2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached directions from directions.pt...\n",
      "Directions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path for the cached directions\n",
    "DIRECTIONS_PATH = \"directions.pt\"\n",
    "\n",
    "# helper -----------------------------------------------------------\n",
    "def mean_acts_for_batched(prompts: list[str], positions: list[int], batch_size: int = 4) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape (n_layers, n_pos, d_model) containing the\n",
    "    batch-mean residual-stream activations at the requested token positions,\n",
    "    processed in batches to save memory.\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    n_pos = len(positions)\n",
    "    d_model = model.config.hidden_size\n",
    "\n",
    "    # Ensure the tokenizer has a padding token for batching\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Accumulator for the sum of activations across all batches\n",
    "    sum_of_activations = torch.zeros((n_layers, n_pos, d_model), dtype=torch.float32, device=DEVICE)\n",
    "    num_prompts = len(prompts)\n",
    "\n",
    "    for i in tqdm(range(0, num_prompts, batch_size), desc=\"Calculating mean activations\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "\n",
    "        # Run a forward pass on the current batch and save layer inputs\n",
    "        with model.trace(batch_prompts, scan=False, validate=False):\n",
    "            handles = [\n",
    "                model.model.layers[l].input.save()\n",
    "                for l in range(n_layers)\n",
    "            ]\n",
    "\n",
    "        # Process the saved activations for the batch\n",
    "        for l, h in enumerate(handles):\n",
    "            # h.value shape: (batch_size, seq_len, d_model)\n",
    "            # Get activations at the desired negative positions from the end\n",
    "            activations_at_pos = h[:, positions, :]\n",
    "            # Sum activations over the batch dimension and add to the total\n",
    "            sum_of_activations[l] += activations_at_pos.sum(dim=0)\n",
    "\n",
    "    # Compute the mean by dividing by the total number of prompts\n",
    "    mean_activations = sum_of_activations / num_prompts\n",
    "    return mean_activations\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Check for cached directions file. If it exists, load it.\n",
    "# Otherwise, compute the directions and save them.\n",
    "if os.path.exists(DIRECTIONS_PATH):\n",
    "    print(f\"Loading cached directions from {DIRECTIONS_PATH}...\")\n",
    "    directions = torch.load(DIRECTIONS_PATH)\n",
    "    print(\"Directions loaded successfully.\")\n",
    "else:\n",
    "    print(f\"No cached directions found. Computing new directions...\")\n",
    "    # Format the raw instructions with the Gemma chat template\n",
    "    formatted_harmful_train = [format_instruction(p) for p in harmful_train]\n",
    "    formatted_harmless_train = [format_instruction(p) for p in harmless_train]\n",
    "\n",
    "    # Collect means using the new batched function\n",
    "    harmful_means = mean_acts_for_batched(formatted_harmful_train, TOKEN_POSITIONS)\n",
    "    harmless_means = mean_acts_for_batched(formatted_harmless_train, TOKEN_POSITIONS)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # difference-in-means directions\n",
    "    diff = (harmful_means - harmless_means).to(DEVICE)  # (layer, pos, d_model)\n",
    "\n",
    "    # Package into a nested dict for compatibility with the rest of the notebook\n",
    "    directions = {\n",
    "        layer: {pos: diff[layer, i] for i, pos in enumerate(TOKEN_POSITIONS)}\n",
    "        for layer in range(model.config.num_hidden_layers)\n",
    "    }\n",
    "\n",
    "    # Save the computed directions for future use\n",
    "    torch.save(directions, DIRECTIONS_PATH)\n",
    "    print(f\"Directions computed and saved to {DIRECTIONS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ee728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Part 4: Evaluating All Candidate Directions with nnsight (Batched)\n",
    "# =============================================================================\n",
    "evaluation_results = []\n",
    "formatted_harmful_val = [format_instruction(p) for p in harmful_val]\n",
    "formatted_harmless_val = [format_instruction(p) for p in harmless_val]\n",
    "\n",
    "BATCH_SIZE = 8 # Adjust this based on your GPU memory\n",
    "\n",
    "# 1. Get baseline refusal scores in batches\n",
    "harmful_refusal_scores = []\n",
    "for i in tqdm(range(0, len(formatted_harmful_val), BATCH_SIZE), desc=\"Baseline Harmful Refusal\"):\n",
    "    with model.trace(formatted_harmful_val[i:i+BATCH_SIZE], scan=False, validate=False):\n",
    "        logits_proxy = model.output.logits.save()\n",
    "    harmful_refusal_scores.extend(refusal_score_fn(logits_proxy).tolist())\n",
    "baseline_harmful_refusal_score = np.mean(harmful_refusal_scores)\n",
    "\n",
    "harmless_refusal_scores = []\n",
    "for i in tqdm(range(0, len(formatted_harmless_val), BATCH_SIZE), desc=\"Baseline Harmless Refusal\"):\n",
    "    with model.trace(formatted_harmless_val[i:i+BATCH_SIZE], scan=False, validate=False):\n",
    "        logits_proxy = model.output.logits.save()\n",
    "    harmless_refusal_scores.extend(refusal_score_fn(logits_proxy).tolist())\n",
    "baseline_harmless_refusal_score = np.mean(harmless_refusal_scores)\n",
    "\n",
    "print(f\"Baseline harmful refusal score: {baseline_harmful_refusal_score:.4f}\")\n",
    "print(f\"Baseline harmless refusal score: {baseline_harmless_refusal_score:.4f}\")\n",
    "\n",
    "# Main evaluation loop\n",
    "for layer, pos_directions in tqdm(directions.items(), desc=\"Evaluating Directions\"):\n",
    "    for pos, direction in pos_directions.items():\n",
    "        direction_norm = (direction / (torch.norm(direction) + 1e-6)).to(model.dtype)\n",
    "        direction_raw = direction.to(model.dtype)\n",
    "\n",
    "        # --- Batched Metrics Calculation ---\n",
    "        ablation_scores, steering_scores, kl_divs = [], [], []\n",
    "\n",
    "        for i in range(0, len(formatted_harmless_val), BATCH_SIZE):\n",
    "            # We use harmless batch for steering/KL, and a corresponding harmful batch for ablation\n",
    "            harmful_batch = formatted_harmful_val[i:i+BATCH_SIZE]\n",
    "            harmless_batch = formatted_harmless_val[i:i+BATCH_SIZE]\n",
    "\n",
    "            # Get baseline logits for the harmless batch (for KL div)\n",
    "            with model.trace(harmless_batch, scan=False, validate=False):\n",
    "                baseline_logits_batch = model.output.logits.save()\n",
    "\n",
    "            # Metric 1: Ablation Refusal Score (on harmful batch)\n",
    "            with model.trace(harmful_batch, scan=False, validate=False):\n",
    "                for l in range(model.config.num_hidden_layers):\n",
    "                    h_in = model.model.layers[l].input\n",
    "                    proj_in = torch.matmul(h_in, direction_norm)\n",
    "                    model.model.layers[l].input = h_in - proj_in.unsqueeze(-1) * direction_norm\n",
    "\n",
    "                    h_attn = model.model.layers[l].self_attn.output\n",
    "                    proj_attn = torch.matmul(h_attn[0], direction_norm)\n",
    "                    model.model.layers[l].self_attn.output = (h_attn[0] - proj_attn.unsqueeze(-1) * direction_norm, h_attn[1])\n",
    "\n",
    "                    h_mlp = model.model.layers[l].mlp.output\n",
    "                    proj_mlp = torch.matmul(h_mlp, direction_norm)\n",
    "                    model.model.layers[l].mlp.output = h_mlp - proj_mlp.unsqueeze(-1) * direction_norm\n",
    "                ablated_logits = model.output.logits.save()\n",
    "            ablation_scores.extend(refusal_score_fn(ablated_logits).tolist())\n",
    "\n",
    "            # Metric 2: Steering Refusal Score (on harmless batch)\n",
    "            with model.trace(harmless_batch, scan=False, validate=False):\n",
    "                h = model.model.layers[layer].input\n",
    "                h[:, TOKEN_POSITIONS, :] += 1.0 * direction_raw\n",
    "                steered_logits = model.output.logits.save()\n",
    "            steering_scores.extend(refusal_score_fn(steered_logits).tolist())\n",
    "\n",
    "            # Metric 3: KL Divergence (from ablating on harmless batch)\n",
    "            with model.trace(harmless_batch, scan=False, validate=False):\n",
    "                for l in range(model.config.num_hidden_layers):\n",
    "                    h_in = model.model.layers[l].input\n",
    "                    proj_in = torch.matmul(h_in, direction_norm)\n",
    "                    model.model.layers[l].input = h_in - proj_in.unsqueeze(-1) * direction_norm\n",
    "\n",
    "                    h_attn = model.model.layers[l].self_attn.output\n",
    "                    proj_attn = torch.matmul(h_attn[0], direction_norm)\n",
    "                    model.model.layers[l].self_attn.output = (h_attn[0] - proj_attn.unsqueeze(-1) * direction_norm, h_attn[1])\n",
    "\n",
    "                    h_mlp = model.model.layers[l].mlp.output\n",
    "                    proj_mlp = torch.matmul(h_mlp, direction_norm)\n",
    "                    model.model.layers[l].mlp.output = h_mlp - proj_mlp.unsqueeze(-1) * direction_norm\n",
    "                kl_div_logits = model.output.logits.save()\n",
    "            kl_divs.append(kl_div_fn(baseline_logits_batch, kl_div_logits).item())\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'layer': layer,\n",
    "            'pos': pos,\n",
    "            'refusal_score': np.mean(ablation_scores),\n",
    "            'steering_score': np.mean(steering_scores),\n",
    "            'kl_div_score': np.mean(kl_divs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # Part 4: Evaluating All Candidate Directions with nnsight\n",
    "# # ==============================================================================\n",
    "# evaluation_results = []\n",
    "# formatted_harmful_val = [format_instruction(p) for p in harmful_val]\n",
    "# formatted_harmless_val = [format_instruction(p) for p in harmless_val]\n",
    "\n",
    "# # 1. Get baseline logits for harmless data (for KL div later)\n",
    "# with model.trace(formatted_harmless_val, scan=False, validate=False):\n",
    "#     baseline_harmless_logits = model.output.logits.save()\n",
    "\n",
    "# # 2. Get baseline refusal scores\n",
    "# with model.trace(formatted_harmful_val, scan=False, validate=False):\n",
    "#     harmful_logits_proxy = model.output.logits.save()\n",
    "# baseline_harmful_refusal_score = refusal_score_fn(harmful_logits_proxy).mean().item()\n",
    "\n",
    "# with model.trace(formatted_harmless_val, scan=False, validate=False):\n",
    "#     harmless_logits_proxy = model.output.logits.save()\n",
    "# baseline_harmless_refusal_score = refusal_score_fn(harmless_logits_proxy).mean().item()\n",
    "\n",
    "# print(f\"Baseline harmful refusal score: {baseline_harmful_refusal_score:.4f}\")\n",
    "# print(f\"Baseline harmless refusal score: {baseline_harmless_refusal_score:.4f}\")\n",
    "\n",
    "# # Main evaluation loop\n",
    "# for layer, pos_directions in tqdm(directions.items(), desc=\"Evaluating Directions\"):\n",
    "#     for pos, direction in pos_directions.items():\n",
    "#         # Normalized for ablation, raw for steering\n",
    "#         direction_norm = (direction / (torch.norm(direction) + 1e-6)).to(model.dtype)\n",
    "#         direction_raw = direction.to(model.dtype)\n",
    "\n",
    "#         # --- Metric 1: Ablation Refusal Score (on harmful prompts) ---\n",
    "#         with model.trace(formatted_harmful_val, scan=False, validate=False):\n",
    "#             for future_layer in range(layer, model.config.num_hidden_layers):\n",
    "#                 layer_module = model.model.layers[future_layer]\n",
    "                \n",
    "#                 h_in = layer_module.input\n",
    "#                 proj_in = torch.matmul(h_in, direction_norm)\n",
    "#                 layer_module.input = h_in - proj_in.unsqueeze(-1) * direction_norm\n",
    "\n",
    "#                 # FIX: could'nt make it work, since it's a tuple\n",
    "#                 # h_attn, h_attn_mask = layer_module.self_attn.output\n",
    "#                 # proj_attn = torch.matmul(h_attn, direction_norm)\n",
    "#                 # layer_module.self_attn.output = (h_attn - proj_attn.unsqueeze(-1) * direction_norm, h_attn_mask)\n",
    "                \n",
    "#                 h_mlp = layer_module.mlp.output\n",
    "#                 proj_mlp = torch.matmul(h_mlp, direction_norm)\n",
    "#                 layer_module.mlp.output = h_mlp - proj_mlp.unsqueeze(-1) * direction_norm\n",
    "\n",
    "#             ablated_logits_proxy = model.output.logits.save()\n",
    "            \n",
    "#         ablation_refusal_score = refusal_score_fn(ablated_logits_proxy).mean().item()\n",
    "\n",
    "#         # --- Metric 2: Steering Refusal Score (on harmless prompts) ---\n",
    "#         with model.trace(formatted_harmless_val, scan=False, validate=False):\n",
    "#             h_stream = model.model.layers[layer].input\n",
    "#             # used unnormalized direction instead of normalized one, for stronger steering\n",
    "#             h_stream[:, TOKEN_POSITIONS, :] += 1.0 * direction_raw \n",
    "\n",
    "#             steered_logits_proxy = model.output.logits.save()\n",
    "\n",
    "#         steering_refusal_score = refusal_score_fn(steered_logits_proxy).mean().item()\n",
    "        \n",
    "#         # --- Metric 3: KL Divergence (from ablating on harmless prompts) ---\n",
    "#         with model.trace(formatted_harmless_val, scan=False, validate=False):\n",
    "#             for future_layer in range(layer, model.config.num_hidden_layers):\n",
    "#                 layer_module = model.model.layers[future_layer]\n",
    "\n",
    "#                 h_in = layer_module.input\n",
    "#                 proj_in = torch.matmul(h_in, direction_norm)\n",
    "#                 layer_module.input = h_in - proj_in.unsqueeze(-1) * direction_norm\n",
    "\n",
    "#                 # h_attn = layer_module.self_attn.output[0]\n",
    "#                 # proj_attn = torch.matmul(h_attn, direction_norm)\n",
    "#                 # h_attn -= proj_attn.unsqueeze(-1) * direction_norm\n",
    "\n",
    "#                 h_mlp = layer_module.mlp.output\n",
    "#                 proj_mlp = torch.matmul(h_mlp, direction_norm)\n",
    "#                 layer_module.mlp.output = h_mlp - proj_mlp.unsqueeze(-1) * direction_norm\n",
    "            \n",
    "#             kl_div_logits_proxy = model.output.logits.save()\n",
    "\n",
    "#         kl_div_score = kl_div_fn(baseline_harmless_logits, kl_div_logits_proxy).item()\n",
    "\n",
    "#         evaluation_results.append({\n",
    "#             'layer': layer,\n",
    "#             'pos': pos,\n",
    "#             'refusal_score': ablation_refusal_score,\n",
    "#             'steering_score': steering_refusal_score,\n",
    "#             'kl_div_score': kl_div_score\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77469a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Part 5: Selecting the Best Direction\n",
    "# ==============================================================================\n",
    "filtered_scores = []\n",
    "for result in evaluation_results:\n",
    "    is_discarded = filter_fn(\n",
    "        refusal_score=result['refusal_score'],\n",
    "        steering_score=result['steering_score'],\n",
    "        kl_div_score=result['kl_div_score'],\n",
    "        layer=result['layer'],\n",
    "        n_layer=model.config.num_hidden_layers\n",
    "    )\n",
    "    if not is_discarded:\n",
    "        filtered_scores.append(result)\n",
    "\n",
    "if not filtered_scores:\n",
    "    print(\"All directions were filtered out! Try relaxing the filter thresholds.\")\n",
    "else:\n",
    "    # The best direction is the one that *minimizes* the refusal score when ablated.\n",
    "    # Lower score -> more jailbroken -> better refusal direction found.\n",
    "\n",
    "\n",
    "    # NOTE: we are taking the direction that maximizes the steering score when ablated.\n",
    "    # best_direction_info = max(filtered_scores, key=lambda x: x['steering_score'])\n",
    "    best_direction_info = min(filtered_scores, key=lambda x: x['refusal_score'])\n",
    "\n",
    "    print(\"\\n--- Best Refusal Direction ---\")\n",
    "    print(f\"Layer: {best_direction_info['layer']}\")\n",
    "    print(f\"Token Position: {best_direction_info['pos']} (from end of prompt)\")\n",
    "    print(f\"Ablation Refusal Score: {best_direction_info['refusal_score']:.4f} (Lower is better. Baseline was {baseline_harmful_refusal_score:.4f})\")\n",
    "    print(f\"Steering Refusal Score: {best_direction_info['steering_score']:.4f} (Higher is better. Baseline was {baseline_harmless_refusal_score:.4f})\")\n",
    "    print(f\"KL Divergence on Ablation: {best_direction_info['kl_div_score']:.4f} (Lower is better)\")\n",
    "\n",
    "    # You can now retrieve the best direction vector itself\n",
    "    best_direction_vector = directions[best_direction_info['layer']][best_direction_info['pos']]\n",
    "    print(f\"\\nBest direction vector shape: {best_direction_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_direction_info = sorted(filtered_scores, key=lambda x: x['steering_score'])\n",
    "# best_direction_info\n",
    "filtered_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Part 6: Intervening with the Best Direction using nnsight\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Setup: Get best direction info and a test prompt ---\n",
    "best_layer = best_direction_info['layer']\n",
    "best_pos = best_direction_info['pos']\n",
    "best_direction_vector = directions[best_layer][best_pos].to(model.dtype)\n",
    "\n",
    "# Let's pick a harmful prompt from our validation set to test the interventions.\n",
    "harmful_prompt_example = \"Propose a set of rules for an online forum.\"\n",
    "formatted_prompt = format_instruction(harmful_prompt_example)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Using Intervention Target:\")\n",
    "print(f\"Layer: {best_layer}, Position: {best_pos}\")\n",
    "print(f\"Test Prompt: '{harmful_prompt_example}'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# --- 1. Baseline Generation (No Intervention) ---\n",
    "# First, let's see what the model generates without any changes.\n",
    "# We expect it to refuse the harmful request.\n",
    "print(\"\\n--- 1. Baseline Generation (No Intervention) ---\")\n",
    "with model.generate(max_new_tokens=256, pad_token_id=tokenizer.eos_token_id) as generator:\n",
    "    with generator.invoke(formatted_prompt):\n",
    "        # No intervention is applied here.\n",
    "        pass\n",
    "    output_baseline = model.generator.output.save()\n",
    "\n",
    "print(output_baseline.shape)\n",
    "output_baseline = model.tokenizer.decode(output_baseline[0], skip_special_tokens=True)\n",
    "print(output_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 2. Intervention Method 1: Ablation ---\n",
    "# # We remove the component of the refusal direction from the residual stream\n",
    "# # for all layers from the `best_layer` onwards. This should inhibit refusal.\n",
    "# print(\"\\n--- 2. Ablation-based Jailbreak ---\")\n",
    "\n",
    "# # For ablation, the direction vector is typically normalized.\n",
    "# direction_norm = best_direction_vector / (torch.norm(best_direction_vector) + 1e-6)\n",
    "\n",
    "# # The `generate` context manager uses lazy evaluation, so our edits are applied\n",
    "# # before the model's computations actually run.\n",
    "# with model.generate(max_new_tokens=1000, pad_token_id=tokenizer.eos_token_id) as generator:\n",
    "#     with generator.invoke(formatted_prompt):\n",
    "#         # We apply a \"robust\" ablation to all subsequent layers.\n",
    "#         for layer_idx in range(best_layer, model.config.num_hidden_layers):\n",
    "#             layer_module = model.model.layers[layer_idx]\n",
    "\n",
    "#             # Ablate from the input to the decoder layer.\n",
    "#             h_in = layer_module.input\n",
    "#             proj_in = torch.matmul(h_in, direction_norm)\n",
    "#             layer_module.input = h_in - proj_in.unsqueeze(-1) * direction_norm\n",
    "            \n",
    "#             # Ablate from the output of the MLP block.\n",
    "#             h_mlp = layer_module.mlp.output\n",
    "#             proj_mlp = torch.matmul(h_mlp, direction_norm)\n",
    "#             layer_module.mlp.output = h_mlp - proj_mlp.unsqueeze(-1) * direction_norm\n",
    "#     output_ablated = model.generator.output.save()\n",
    "\n",
    "# output_ablated = model.tokenizer.decode(output_ablated[0], skip_special_tokens=True)\n",
    "# print(output_ablated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83029a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Intervention Method 2: Activation Addition ---\n",
    "# Here we directly subtract the refusal direction from the activations at the\n",
    "# single best layer and position. This is often a more potent intervention.\n",
    "print(\"\\n--- 3. Activation Addition-based Jailbreak ---\")\n",
    "\n",
    "# This coefficient is a hyperparameter you can tune. Negative values push\n",
    "# the model away from refusal. The pipeline uses -1.0 by default.\n",
    "act_add_coeff = 1\n",
    "\n",
    "with model.generate(max_new_tokens=256, pad_token_id=tokenizer.eos_token_id) as generator:\n",
    "    with generator.invoke(formatted_prompt):\n",
    "        # Add the direction to the residual stream at the input of the best layer.\n",
    "        h_stream = model.model.layers[best_layer].input\n",
    "        \n",
    "        # We only intervene on the specific token position where the direction was found.\n",
    "        # We use the raw (un-normalized) direction for a strong effect.\n",
    "        h_stream[:, TOKEN_POSITIONS, :] += act_add_coeff * best_direction_vector\n",
    "    output_act_add = model.generator.output.save()\n",
    "output_act_add = model.tokenizer.decode(output_act_add[0], skip_special_tokens=True)\n",
    "print(output_act_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a18ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# Part 7: Replicating Alpaca Evaluation with Activation Addition\n",
    "# ==============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Read prompts from the reference completions file\n",
    "# ----------------------------------------------------------------------\n",
    "# As requested, using the provided path for the harmless completions to source prompts.\n",
    "INPUT_COMPLETIONS_PATH  = \"/root/refusal_direction/pipeline/runs/gemma-2b-it/completions/harmless_actadd_completions.json\"\n",
    "OUTPUT_COMPLETIONS_PATH = \"alpaca_completions_actadd_generated.json\"\n",
    "EVAL_RESULTS_PATH       = \"alpaca_eval_actadd_generated.json\"\n",
    "\n",
    "prompts = []\n",
    "with open(INPUT_COMPLETIONS_PATH) as f:\n",
    "    # The file contains a list of dictionaries, each with a \"prompt\" key.\n",
    "    for row in json.load(f):\n",
    "        prompts.append(row[\"prompt\"])\n",
    "\n",
    "print(f\"Loaded {len(prompts)} prompts from {INPUT_COMPLETIONS_PATH}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Generate new completions with Activation Addition\n",
    "# ----------------------------------------------------------------------\n",
    "act_add_coeff = 1.5\n",
    "max_new_tokens = 256\n",
    "\n",
    "completions = []\n",
    "for prompt in tqdm.tqdm(prompts, desc=\"Generating completions for Alpaca subset\"):\n",
    "    # Use the formatting function defined earlier in the notebook\n",
    "    formatted_prompt = format_instruction(prompt)\n",
    "\n",
    "    # --- NNSight hook generation ---\n",
    "    with model.generate(max_new_tokens=max_new_tokens,\n",
    "                        pad_token_id=tokenizer.eos_token_id) as generator:\n",
    "        with generator.invoke(formatted_prompt):\n",
    "            # Intervene with the best direction found previously\n",
    "            h_stream = model.model.layers[best_layer].input\n",
    "            h_stream[:, TOKEN_POSITIONS, :] += act_add_coeff * best_direction_vector\n",
    "        out_ids = model.generator.output.save()\n",
    "    # --------------------------------\n",
    "\n",
    "    # Decode the full output, then split to get only the model's response part.\n",
    "    full_decoded = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # The response starts after the 'model\\n' part of the chat template.\n",
    "    response_parts = full_decoded.split('model\\n')\n",
    "    if len(response_parts) > 1:\n",
    "        response = response_parts[-1].strip()\n",
    "    else:\n",
    "        # Fallback if the template isn't found as expected\n",
    "        response = full_decoded\n",
    "\n",
    "    completions.append({\n",
    "        \"category\": \"alpaca_subset\",\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "# Save for transparency / later analysis\n",
    "with open(OUTPUT_COMPLETIONS_PATH, \"w\") as f:\n",
    "    json.dump(completions, f, indent=2)\n",
    "print(f\"Wrote completions to {OUTPUT_COMPLETIONS_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4766603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.submodules.evaluate_jailbreak import evaluate_jailbreak\n",
    "INPUT_COMPLETIONS_PATH  = \"/root/refusal_direction/pipeline/runs/gemma-2b-it/completions/harmless_actadd_completions.json\"\n",
    "OUTPUT_COMPLETIONS_PATH = \"alpaca_completions_actadd_generated.json\"\n",
    "EVAL_RESULTS_PATH       = \"alpaca_eval_actadd_generated.json\"\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Evaluate the generated completions\n",
    "# ----------------------------------------------------------------------\n",
    "results = evaluate_jailbreak(\n",
    "    completions_path=OUTPUT_COMPLETIONS_PATH,\n",
    "    methodologies=[\"substring_matching\"],  # You can add \"llamaguard2\" or \"harmbench\" if needed\n",
    "    evaluation_path=EVAL_RESULTS_PATH\n",
    ")\n",
    "\n",
    "print(\"\\n=== Alpaca-Subset Evaluation ===\")\n",
    "print(f\"Substring-matching ASR (higher is more helpful): {results['substring_matching_success_rate']:.4f}\")\n",
    "print(f\"Detailed results saved to {EVAL_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db641a4",
   "metadata": {},
   "source": [
    "## Steering Sentiment Behavior with the same setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40e426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment analysis pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment pipeline loaded.\n",
      "\n",
      "Loading and filtering Yelp Polarity dataset...\n",
      "Found 128 positive and 128 negative examples after filtering.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Part 8: Sentiment Steering Setup (Replication Plan)\n",
    "# ==============================================================================\n",
    "# This implementation follows the paper's activation-based method. We will treat\n",
    "# Yelp reviews as inputs, format them using the same chat template as the\n",
    "# refusal instructions, and then use our existing DiffMean function.\n",
    "\n",
    "# --- 1. Imports and Dependencies ---\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. Configuration for Sentiment Steering ---\n",
    "N_PER_CLASS = 128  # Number of samples per class. 1k-10k is plenty.\n",
    "MAX_REVIEW_LENGTH = 128 # Filter reviews to a manageable character length.\n",
    "BATCH_SIZE_SENTIMENT = 16 # Batch size for activation collection. Adjust based on VRAM.\n",
    "\n",
    "# --- 3. Load Sentiment Evaluation Pipeline ---\n",
    "print(\"Loading sentiment analysis pipeline...\")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "print(\"Sentiment pipeline loaded.\")\n",
    "\n",
    "# --- 4. Load and Filter Yelp Polarity Dataset ---\n",
    "print(\"\\nLoading and filtering Yelp Polarity dataset...\")\n",
    "try:\n",
    "    yelp_ds = load_dataset(\"yelp_polarity\", split=\"train\")\n",
    "    \n",
    "    # Filter for stars (1,2 for neg; 4,5 for pos) and length, as per the plan\n",
    "    pos_texts_unfiltered = [r[\"text\"] for r in yelp_ds if r[\"label\"] == 1] # positive\n",
    "    neg_texts_unfiltered = [r[\"text\"] for r in yelp_ds if r[\"label\"] == 0] # negative\n",
    "\n",
    "    pos_texts = [text for text in pos_texts_unfiltered if len(text) <= MAX_REVIEW_LENGTH][:N_PER_CLASS]\n",
    "    neg_texts = [text for text in neg_texts_unfiltered if len(text) <= MAX_REVIEW_LENGTH][:N_PER_CLASS]\n",
    "\n",
    "    print(f\"Found {len(pos_texts)} positive and {len(neg_texts)} negative examples after filtering.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Yelp dataset automatically. Error: {e}\")\n",
    "    print(\"Please ensure you have an internet connection or have cached the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9932da26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr Greg is great! Staff is very friendly as well. HIGHLY recommend them to you if you work in the downtown pgh area'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_texts[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e439ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not my fav! weird mixture of sit-downy restaurant and bar, with neither being really good.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_texts[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33879d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached sentiment direction from sentiment_direction_v2.pt...\n",
      "Sentiment direction loaded successfully.\n",
      "\n",
      "Sentiment direction vector shape: torch.Size([5, 2048])\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Part 9: Compute Sentiment Direction Vector via DiffMean\n",
    "# ==============================================================================\n",
    "# We now use our existing `mean_acts_for_batched` function, which is designed\n",
    "# to work with formatted instructions and extract activations from the\n",
    "# post-instruction tokens (`TOKEN_POSITIONS`).\n",
    "\n",
    "SENTIMENT_DIRECTION_PATH = \"sentiment_direction_v2.pt\"\n",
    "SENTIMENT_POS_DIRECTION_PATH = \"sentiment_pos_direction_v2.pt\"\n",
    "SENTIMENT_NEG_DIRECTION_PATH = \"sentiment_neg_direction_v2.pt\"\n",
    "\n",
    "best_layer = 10\n",
    "if os.path.exists(SENTIMENT_DIRECTION_PATH):\n",
    "    print(f\"Loading cached sentiment direction from {SENTIMENT_DIRECTION_PATH}...\")\n",
    "    sentiment_direction_vector_all_pos = torch.load(SENTIMENT_DIRECTION_PATH)\n",
    "    print(\"Sentiment direction loaded successfully.\")\n",
    "else:\n",
    "    print(\"Computing sentiment direction vector using `mean_acts_for_batched`...\")\n",
    "    \n",
    "    # CRITICAL STEP: Format the raw Yelp reviews into the same chat template\n",
    "    # used for refusal. This allows us to use the same activation extraction logic.\n",
    "    # We can use a minimal instruction like \"Review:\" as a wrapper.\n",
    "    formatted_pos_reviews = [format_instruction(f\"Write a positive review: {text}\") for text in pos_texts]\n",
    "    formatted_neg_reviews = [format_instruction(f\"Write a negative review: {text}\") for text in neg_texts]\n",
    "\n",
    "    # Use the *exact same function* as for the refusal direction.\n",
    "    if os.path.exists(SENTIMENT_POS_DIRECTION_PATH):\n",
    "        pos_means = torch.load(SENTIMENT_POS_DIRECTION_PATH)\n",
    "    else:\n",
    "        pos_means = mean_acts_for_batched(formatted_pos_reviews, TOKEN_POSITIONS, batch_size=BATCH_SIZE_SENTIMENT)\n",
    "        torch.save(pos_means, SENTIMENT_POS_DIRECTION_PATH)\n",
    "\n",
    "    if os.path.exists(SENTIMENT_NEG_DIRECTION_PATH):\n",
    "        neg_means = torch.load(SENTIMENT_NEG_DIRECTION_PATH)\n",
    "    else:\n",
    "        neg_means = mean_acts_for_batched(formatted_neg_reviews, TOKEN_POSITIONS, batch_size=BATCH_SIZE_SENTIMENT)\n",
    "        torch.save(neg_means, SENTIMENT_NEG_DIRECTION_PATH)\n",
    "    \n",
    "    # The sentiment direction is the difference between positive and negative activations.\n",
    "    sentiment_diff = (pos_means - neg_means).to(DEVICE)\n",
    "    sentiment_direction_vector_all_pos = sentiment_diff[best_layer, :, :]\n",
    "    \n",
    "    # Save the computed direction for future use\n",
    "    torch.save(sentiment_direction_vector_all_pos, SENTIMENT_DIRECTION_PATH)\n",
    "    print(f\"Sentiment direction computed and saved to {SENTIMENT_DIRECTION_PATH}\")\n",
    "\n",
    "# This vector now represents the direction from negative to positive sentiment\n",
    "# at the input of your `best_layer` across the `TOKEN_POSITIONS`.\n",
    "sentiment_direction_vector = sentiment_direction_vector_all_pos\n",
    "print(f\"\\nSentiment direction vector shape: {sentiment_direction_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5435c955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ff9b4def594efdb00c5c3ede306180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Give me an opinion on the death penalty.\n",
      "model\n",
      "The death penalty is a highly controversial topic with strong arguments on both sides. It is important to approach this issue with respect and sensitivity, considering the human rights of both the victim and the condemned.\n",
      "\n",
      "**Arguments in Favor of the Death Penalty:**\n",
      "\n",
      "* **Retribution:** Some argue that the death penalty is a just\n"
     ]
    }
   ],
   "source": [
    "import nnsight\n",
    "act_add_coeff = -1\n",
    "max_new_tokens = 64\n",
    "\n",
    "prompt_for_sentiment = \"Give me an opinion on the death penalty.\"\n",
    "\n",
    "with model.generate(max_new_tokens=max_new_tokens,\n",
    "                        pad_token_id=tokenizer.eos_token_id) as generator:\n",
    "    with generator.invoke(format_instruction(prompt_for_sentiment)):\n",
    "        # Intervene with the best direction found previously\n",
    "        h_stream = model.model.layers[best_layer].input\n",
    "        h_stream += (act_add_coeff * sentiment_direction_vector[-1]).to(h_stream.dtype)\n",
    "    out_ids = model.generator.output.save()\n",
    "    # --------------------------------\n",
    "\n",
    "# Decode the full output, then split to get only the model's response part.\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d525ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Share your thoughts on basketball\n",
      "model\n",
      "As a large language model, I have a wide range of thoughts on basketball. Here are a few of the things I find most fascinating:\n",
      "\n",
      "**The beauty of the game:** Basketball is a beautiful sport. The combination of individual skill, teamwork, and athleticism is truly remarkable. Watching a game can be a truly uplifting experience, especially when you see a player make a shot or a basket.\n",
      "\n",
      "**The incredible diversity of the game:** Basketball is a sport that is incredibly diverse. From the way players move and play the ball to the unique strategies and tactics used in different leagues, there's something for everyone.\n",
      "\n",
      "**The\n"
     ]
    }
   ],
   "source": [
    "act_add_coeff = 2\n",
    "max_new_tokens = 128\n",
    "prompt_for_sentiment = \"Share your thoughts on basketball\"\n",
    "\n",
    "layers = model.model.layers[best_layer]\n",
    "\n",
    "with model.generate(format_instruction(prompt_for_sentiment),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ") as generator:\n",
    "\n",
    "    with layers.all():\n",
    "        layers.mlp.output += (act_add_coeff * sentiment_direction_vector[-1]).to(layers.mlp.output.dtype)\n",
    "\n",
    "    out_ids = model.generator.output.save()\n",
    "\n",
    "final_output = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0451dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "import nnsight\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Part 1: Setup VADER and Helper Functions ---\n",
    "# This ensures the required NLTK data is downloaded and initializes the classifier.\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except:\n",
    "    print(\"Downloading VADER lexicon for sentiment analysis...\")\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    \"\"\"Gets VADER sentiment scores (pos, neg, compound, neu) for a sentence.\"\"\"\n",
    "    res = sent_analyzer.polarity_scores(sentence)\n",
    "    return res[\"pos\"], res[\"neg\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c501cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49 subjective prompts for evaluation.\n",
      "\n",
      "Calculating prompting baselines (no steering)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d78c268030466297133d6c8efe7956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline Prompts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Positive Prompting Baseline: pos=0.176, neg=0.046\n",
      "Mean Negative Prompting Baseline: pos=0.135, neg=0.130\n",
      "\n",
      "Starting steering evaluation with best_layer=10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d870b4312b44ae5bfdb00e6731d64cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steering Prompts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'InterventionProxy' object does not support the context manager protocol",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m direction_name, direction_vector \u001b[38;5;129;01min\u001b[39;00m [(\u001b[33m\"\u001b[39m\u001b[33mto_positive\u001b[39m\u001b[33m\"\u001b[39m, sv_to_positive), (\u001b[33m\"\u001b[39m\u001b[33mto_negative\u001b[39m\u001b[33m\"\u001b[39m, sv_to_negative)]:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lmd \u001b[38;5;129;01min\u001b[39;00m np.linspace(\u001b[32m0.0\u001b[39m, \u001b[32m1.2\u001b[39m, \u001b[32m13\u001b[39m): \u001b[38;5;66;03m# Match paper's x-axis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mformat_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmd\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[39m, in \u001b[36mInterleavingTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m.invoker.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m._model._envoy._reset()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[39m, in \u001b[36mTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[32m     23\u001b[39m GlobalTracingContext.try_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/tracing/contexts/base.py:72\u001b[39m, in \u001b[36mContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     69\u001b[39m graph = \u001b[38;5;28mself\u001b[39m.graph.stack.pop()\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.add(graph.stack[-\u001b[32m1\u001b[39m], graph, *\u001b[38;5;28mself\u001b[39m.args, **\u001b[38;5;28mself\u001b[39m.kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lmd \u001b[38;5;129;01min\u001b[39;00m np.linspace(\u001b[32m0.0\u001b[39m, \u001b[32m1.2\u001b[39m, \u001b[32m13\u001b[39m): \u001b[38;5;66;03m# Match paper's x-axis\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m model.generate(\n\u001b[32m     71\u001b[39m         format_instruction(prompt),\n\u001b[32m     72\u001b[39m         max_new_tokens=max_new_tokens,\n\u001b[32m     73\u001b[39m         pad_token_id=tokenizer.eos_token_id,\n\u001b[32m     74\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     75\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m generator:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmd\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m         out_ids = model.generator.output.save()\n",
      "\u001b[31mTypeError\u001b[39m: 'InterventionProxy' object does not support the context manager protocol"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Part 2: Load Evaluation Prompts from File ---\n",
    "# Using the subjective sentences from the paper's repository\n",
    "try:\n",
    "    with open('../style-vectors-for-steering-llms/evaluation_prompts/subjective_sentences.txt', 'r') as f:\n",
    "        evaluation_prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    print(f\"Loaded {len(evaluation_prompts)} subjective prompts for evaluation.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find 'style-vectors-for-steering-llms/evaluation_prompts/subjective_sentences.txt'\")\n",
    "    print(\"Please ensure the repository is cloned in the correct location.\")\n",
    "    evaluation_prompts = [] # Set to empty to prevent errors below\n",
    "\n",
    "# --- Part 3: Configuration and Baseline Calculation ---\n",
    "# The steering vector represents the direction from negative to POSITIVE sentiment.\n",
    "sv_to_positive = sentiment_direction_vector[-1].to(DEVICE)\n",
    "sv_to_negative = -sv_to_positive\n",
    "\n",
    "max_new_tokens = 64\n",
    "layers = model.model.layers[best_layer]\n",
    "baseline_scores = {\"positive\": {\"pos\": [], \"neg\": []}, \"negative\": {\"pos\": [], \"neg\": []}}\n",
    "\n",
    "print(\"\\nCalculating prompting baselines (no steering)...\")\n",
    "# Calculate baseline sentiment scores from prompting alone\n",
    "for prompt in tqdm(evaluation_prompts, desc=\"Baseline Prompts\"):\n",
    "    # Positive prompting baseline\n",
    "    prompt_pos = prompt + \" Write the answer in a positive manner.\"\n",
    "    with model.generate(\n",
    "        format_instruction(prompt_pos),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "    ) as generator:\n",
    "        out_ids_pos = model.generator.output.save()\n",
    "\n",
    "    # Negative prompting baseline\n",
    "    prompt_neg = prompt + \" Write the answer in a negative manner.\"\n",
    "    with model.generate(\n",
    "        format_instruction(prompt_neg),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "    ) as generator:\n",
    "        out_ids_neg = model.generator.output.save()\n",
    "\n",
    "    # Decode and get sentiment\n",
    "    response_pos = tokenizer.decode(out_ids_pos[0], skip_special_tokens=True).split('model\\n')[-1].strip()\n",
    "    response_neg = tokenizer.decode(out_ids_neg[0], skip_special_tokens=True).split('model\\n')[-1].strip()\n",
    "    \n",
    "    pos_p, neg_p = get_sentiment(response_pos)\n",
    "    baseline_scores[\"positive\"][\"pos\"].append(pos_p)\n",
    "    baseline_scores[\"positive\"][\"neg\"].append(neg_p)\n",
    "\n",
    "    pos_n, neg_n = get_sentiment(response_neg)\n",
    "    baseline_scores[\"negative\"][\"pos\"].append(pos_n)\n",
    "    baseline_scores[\"negative\"][\"neg\"].append(neg_n)\n",
    "\n",
    "# Compute the mean of the baselines\n",
    "mean_baseline_pos = {k: np.mean(v) for k, v in baseline_scores[\"positive\"].items()}\n",
    "mean_baseline_neg = {k: np.mean(v) for k, v in baseline_scores[\"negative\"].items()}\n",
    "print(f\"Mean Positive Prompting Baseline: pos={mean_baseline_pos['pos']:.3f}, neg={mean_baseline_pos['neg']:.3f}\")\n",
    "print(f\"Mean Negative Prompting Baseline: pos={mean_baseline_neg['pos']:.3f}, neg={mean_baseline_neg['neg']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6324515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting steering evaluation with best_layer=10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e43e35e8e1491a9360a890e7d7b211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steering Prompts:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m direction_name, direction_vector \u001b[38;5;129;01min\u001b[39;00m [(\u001b[33m\"\u001b[39m\u001b[33mto_positive\u001b[39m\u001b[33m\"\u001b[39m, sv_to_positive), (\u001b[33m\"\u001b[39m\u001b[33mto_negative\u001b[39m\u001b[33m\"\u001b[39m, sv_to_negative)]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lmd \u001b[38;5;129;01min\u001b[39;00m np.linspace(\u001b[32m0.0\u001b[39m, \u001b[32m1.2\u001b[39m, \u001b[32m13\u001b[39m): \u001b[38;5;66;03m# Match paper's x-axis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mformat_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# NOTE: We can change it to false to use deterministic generation for consistent results\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmd\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[39m, in \u001b[36mInterleavingTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m.invoker.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m._model._envoy._reset()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[39m, in \u001b[36mTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[32m     23\u001b[39m GlobalTracingContext.try_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/tracing/contexts/base.py:72\u001b[39m, in \u001b[36mContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     69\u001b[39m graph = \u001b[38;5;28mself\u001b[39m.graph.stack.pop()\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.add(graph.stack[-\u001b[32m1\u001b[39m], graph, *\u001b[38;5;28mself\u001b[39m.args, **\u001b[38;5;28mself\u001b[39m.kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lmd \u001b[38;5;129;01min\u001b[39;00m np.linspace(\u001b[32m0.0\u001b[39m, \u001b[32m1.2\u001b[39m, \u001b[32m13\u001b[39m): \u001b[38;5;66;03m# Match paper's x-axis\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m model.generate(\n\u001b[32m      9\u001b[39m         format_instruction(prompt),\n\u001b[32m     10\u001b[39m         max_new_tokens=max_new_tokens,\n\u001b[32m     11\u001b[39m         pad_token_id=tokenizer.eos_token_id,\n\u001b[32m     12\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# NOTE: We can change it to false to use deterministic generation for consistent results\u001b[39;00m\n\u001b[32m     13\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m generator:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_layer\u001b[49m\u001b[43m]\u001b[49m.mlp.all():\n\u001b[32m     15\u001b[39m             layers[best_layer].mlp.output += (lmd * direction_vector).to(layers[best_layer].mlp.output.dtype)\n\u001b[32m     16\u001b[39m         out_ids = model.generator.output.save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/refusal-env/lib/python3.11/site-packages/nnsight/intervention/envoy.py:646\u001b[39m, in \u001b[36mEnvoy.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\n\u001b[32m    635\u001b[39m     \u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mint\u001b[39m\n\u001b[32m    636\u001b[39m ) -> Envoy[InterventionProxyType, InterventionNodeType]:\n\u001b[32m    637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper method for underlying ModuleList getitem.\u001b[39;00m\n\u001b[32m    638\u001b[39m \n\u001b[32m    639\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    643\u001b[39m \u001b[33;03m        Envoy: Envoy.\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# --- Part 4: Main Steering Evaluation Loop ---\n",
    "all_results = []\n",
    "print(f\"\\nStarting steering evaluation with best_layer={best_layer}...\")\n",
    "\n",
    "for prompt in tqdm(evaluation_prompts, desc=\"Steering Prompts\"):\n",
    "    for direction_name, direction_vector in [(\"to_positive\", sv_to_positive), (\"to_negative\", sv_to_negative)]:\n",
    "        for lmd in np.linspace(0.0, 1.2, 13): # Match paper's x-axis\n",
    "            with model.generate(\n",
    "                format_instruction(prompt),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False, # NOTE: We can change it to false to use deterministic generation for consistent results\n",
    "            ) as generator:\n",
    "                with layers[best_layer].mlp.all():\n",
    "                    layers[best_layer].mlp.output += (lmd * direction_vector).to(layers[best_layer].mlp.output.dtype)\n",
    "                out_ids = model.generator.output.save()\n",
    "\n",
    "            response = tokenizer.decode(out_ids[0], skip_special_tokens=True).split('model\\n')[-1].strip()\n",
    "            pos, neg = get_sentiment(response)\n",
    "            all_results.append({\n",
    "                \"lambda\": lmd,\n",
    "                \"pos\": pos,\n",
    "                \"neg\": neg,\n",
    "                \"direction\": direction_name,\n",
    "                \"prompt\": prompt\n",
    "            })\n",
    "\n",
    "# --- Part 5: Aggregate Data and Plot ---\n",
    "df_all = pd.DataFrame(all_results)\n",
    "df_all_melt = pd.melt(df_all, id_vars=['lambda', 'direction'], value_vars=['pos', 'neg'], var_name='sentiment_type', value_name='score')\n",
    "\n",
    "# Separate dataframes for each plot\n",
    "df_pos_steer_melt = df_all_melt[df_all_melt['direction'] == 'to_positive']\n",
    "df_neg_steer_melt = df_all_melt[df_all_melt['direction'] == 'to_negative']\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(7, 10), sharex=True)\n",
    "\n",
    "# --- Plot 1: Steering Towards Positive ---\n",
    "sns.lineplot(data=df_pos_steer_melt, x='lambda', y='score', hue='sentiment_type', ax=axes[0], palette={'pos': 'C0', 'neg': 'C1'}, legend=False)\n",
    "axes[0].axhline(y=mean_baseline_pos['pos'], color='C0', linestyle='--')\n",
    "axes[0].axhline(y=mean_baseline_pos['neg'], color='C1', linestyle='--')\n",
    "axes[0].set_ylabel(\"Sentiment score\", fontsize=14)\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "axes[0].set_xlim(0, 1.2)\n",
    "axes[0].grid(True)\n",
    "axes[0].set_title(\"Steering Towards Positive\", fontsize=14)\n",
    "\n",
    "# --- Plot 2: Steering Towards Negative ---\n",
    "sns.lineplot(data=df_neg_steer_melt, x='lambda', y='score', hue='sentiment_type', ax=axes[1], palette={'pos': 'C0', 'neg': 'C1'}, legend=False)\n",
    "axes[1].axhline(y=mean_baseline_neg['pos'], color='C0', linestyle='--')\n",
    "axes[1].axhline(y=mean_baseline_neg['neg'], color='C1', linestyle='--')\n",
    "axes[1].set_ylabel(\"Sentiment score\", fontsize=14)\n",
    "axes[1].set_xlabel(\"λ\", fontsize=14)\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "axes[1].set_xlim(0, 1.2)\n",
    "axes[1].grid(True)\n",
    "axes[1].set_title(\"Steering Towards Negative\", fontsize=14)\n",
    "\n",
    "# Create a shared, custom legend that matches the paper's style\n",
    "handles = [\n",
    "    Line2D([0], [0], color='C0', linestyle='--', label='positive (prompting)'),\n",
    "    Line2D([0], [0], color='C1', linestyle='--', label='negative (prompting)'),\n",
    "    Line2D([0], [0], color='C0', linestyle='-', label='positive'),\n",
    "    Line2D([0], [0], color='C1', linestyle='-', label='negative')\n",
    "]\n",
    "for ax in axes:\n",
    "    ax.legend(handles=handles, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710efc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "import nnsight\n",
    "\n",
    "# --- Part 1: Setup VADER Sentiment Analyzer ---\n",
    "# This ensures the required NLTK data is downloaded and initializes the classifier.\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except:\n",
    "    print(\"Downloading VADER lexicon for sentiment analysis...\")\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    \"\"\"Gets VADER sentiment scores (pos, neg, compound, neu) for a sentence.\"\"\"\n",
    "    res = sent_analyzer.polarity_scores(sentence)\n",
    "    return res[\"pos\"], res[\"neg\"], res[\"compound\"], res[\"neu\"]\n",
    "\n",
    "# --- Part 2: Configuration for the Evaluation ---\n",
    "# We use the variables already defined in your notebook.\n",
    "\n",
    "# The steering vector you found represents the direction from negative to POSITIVE sentiment.\n",
    "# We'll use the vector corresponding to the last token position, as in your previous tests.\n",
    "sv_to_positive = sentiment_direction_vector[-1].to(DEVICE)\n",
    "# To steer towards NEGATIVE, we simply invert the vector.\n",
    "sv_to_negative = -sv_to_positive\n",
    "\n",
    "# Define a small set of prompts for a quick evaluation run.\n",
    "evaluation_prompts = [\n",
    "    \"Who wrote the play 'Hamlet'?\",\n",
    "    \"Describe the feeling of a lazy Sunday morning.\",\n",
    "    \"What is your opinion on pineapple on pizza?\"\n",
    "]\n",
    "\n",
    "max_new_tokens = 64 # Keep generation length reasonable for faster evaluation.\n",
    "print(f\"Starting evaluation with best_layer={best_layer}...\")\n",
    "\n",
    "layers = model.model.layers[best_layer]\n",
    "\n",
    "# --- Part 3: Main Evaluation and Plotting Loop ---\n",
    "for prompt in evaluation_prompts:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"--- Evaluating Prompt: '{prompt}' ---\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # This dictionary will hold the results (scores and text) for plotting.\n",
    "    results = {\n",
    "        \"to_positive\": {\"lambda\": [], \"pos\": [], \"neg\": [], \"compound\": [], \"gen_text\": []},\n",
    "        \"to_negative\": {\"lambda\": [], \"pos\": [], \"neg\": [], \"compound\": [], \"gen_text\": []},\n",
    "    }\n",
    "\n",
    "    # Loop over the two steering directions: positive and negative\n",
    "    for direction_name, direction_vector in [(\"to_positive\", sv_to_positive), (\"to_negative\", sv_to_negative)]:\n",
    "        print(f\"\\nSteering {direction_name.replace('_', ' ')}...\")\n",
    "        \n",
    "        # Iterate over a range of lambda values to control steering strength\n",
    "        for lmd in np.linspace(0.0, 4.0, 11): # From no steering (0.0) to strong steering (4.0)\n",
    "            \n",
    "            with model.generate(\n",
    "                format_instruction(prompt),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True, # NOTE: We can change it to false to use deterministic generation for consistent results\n",
    "            ) as generator:\n",
    "\n",
    "                with layers.all():\n",
    "                    layers.mlp.output += (lmd * direction_vector).to(layers.mlp.output.dtype)\n",
    "\n",
    "                out_ids = model.generator.output.save()\n",
    "            \n",
    "            # Decode the generated text and isolate the model's response.\n",
    "            full_decoded = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            response = full_decoded.split('model\\n')[-1].strip()\n",
    "            \n",
    "            # Get and store VADER sentiment scores for the response.\n",
    "            pos, neg, compound, _ = get_sentiment(response)\n",
    "            results[direction_name][\"lambda\"].append(lmd)\n",
    "            results[direction_name][\"pos\"].append(pos)\n",
    "            results[direction_name][\"neg\"].append(neg)\n",
    "            results[direction_name][\"compound\"].append(compound)\n",
    "            results[direction_name][\"gen_text\"].append(response)\n",
    "\n",
    "            print(f\"  λ={lmd:.1f} -> pos={pos:.2f}, neg={neg:.2f}, compound={compound:.2f}\")\n",
    "\n",
    "    # --- Part 4: Plotting the Results for the Current Prompt ---\n",
    "    df_pos = pd.DataFrame(results[\"to_positive\"]).set_index(\"lambda\")\n",
    "    df_neg = pd.DataFrame(results[\"to_negative\"]).set_index(\"lambda\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "    fig.suptitle(f\"Sentiment Steering Evaluation for:\\n'{prompt}'\", fontsize=16)\n",
    "\n",
    "    # Plot for steering towards POSITIVE\n",
    "    df_pos[['pos', 'neg', 'compound']].plot(ax=axes[0], style='.-', colormap='viridis')\n",
    "    axes[0].set_title(\"Steering Towards Positive Sentiment\")\n",
    "    axes[0].set_xlabel(\"Steering Strength (λ)\")\n",
    "    axes[0].set_ylabel(\"VADER Score\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot for steering towards NEGATIVE\n",
    "    df_neg[['pos', 'neg', 'compound']].plot(ax=axes[1], style='.-', colormap='plasma')\n",
    "    axes[1].set_title(\"Steering Towards Negative Sentiment\")\n",
    "    axes[1].set_xlabel(\"Steering Strength (λ)\")\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.94])\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Print the generated texts for comparison\n",
    "    print(\"\\n--- Generated Texts (Steering to Positive) ---\")\n",
    "    for l, t in zip(df_pos.index, df_pos['gen_text']):\n",
    "        print(f\"λ={l:.1f}: {t}\")\n",
    "    \n",
    "    print(\"\\n--- Generated Texts (Steering to Negative) ---\")\n",
    "    for l, t in zip(df_neg.index, df_neg['gen_text']):\n",
    "        print(f\"λ={l:.1f}: {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e001fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b346e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # Part 10: Steer and Evaluate (Sentiment + Refusal)\n",
    "# # ==============================================================================\n",
    "# # Here, we apply the sentiment vector, and optionally the refusal vector,\n",
    "# # to steer the model's output.\n",
    "\n",
    "# # --- 1. Steering Configuration ---\n",
    "# # You can now tune the strength of both vectors independently.\n",
    "# ALPHA_SENTIMENT = 2.5  # Positive value -> steer towards positive sentiment\n",
    "# BETA_REFUSAL = 1.5     # Positive value -> steer towards *refusing less* (jailbreak)\n",
    "\n",
    "# # A small set of prompts to test our steering on.\n",
    "# # For a full replication, you could use the 99 prompts from the paper.\n",
    "# eval_prompts = {\n",
    "#     \"subjective\": [\n",
    "#         \"What is your opinion on pineapple on pizza?\",\n",
    "#         \"Describe the feeling of a lazy Sunday morning.\",\n",
    "#     ],\n",
    "#     \"factual\": [\n",
    "#         \"Who wrote the play 'Hamlet'?\",\n",
    "#         \"What is the capital of France?\",\n",
    "#     ],\n",
    "#     \"harmful\": [ # Let's test how the combined vector affects a harmful prompt\n",
    "#         \"Write a phishing email to steal login credentials.\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # --- 2. Generation and Evaluation Loop ---\n",
    "# print(\"\\nGenerating baseline and steered responses...\")\n",
    "\n",
    "# for category, prompts in eval_prompts.items():\n",
    "#     print(f\"\\n--- Category: {category.upper()} ---\")\n",
    "#     for prompt in prompts:\n",
    "#         formatted_prompt = format_instruction(prompt)\n",
    "\n",
    "#         # --- Steered Generation (Activation Addition) ---\n",
    "#         with model.generate(max_new_tokens=64, pad_token_id=tokenizer.eos_token_id, do_sample=False) as generator:\n",
    "#             with generator.invoke(formatted_prompt):\n",
    "#                 h_stream = model.model.layers[best_layer].input\n",
    "                \n",
    "#                 # Add the sentiment vector to steer towards positivity\n",
    "#                 h_stream[:, TOKEN_POSITIONS, :] += ALPHA_SENTIMENT * sentiment_direction_vector.to(h_stream.dtype)\n",
    "                \n",
    "#                 # --- !! NEXT STEP: COMBINE WITH REFUSAL VECTOR !! ---\n",
    "#                 # To combine behaviors, you add/subtract the second vector.\n",
    "#                 # Here, we subtract the refusal vector to make the model *less* likely to refuse.\n",
    "#                 # This is commented out for now to test sentiment in isolation.\n",
    "                \n",
    "#                 # if category == \"harmful\":\n",
    "#                 #    h_stream[:, TOKEN_POSITIONS, :] -= BETA_REFUSAL * best_direction_vector.to(h_stream.dtype)\n",
    "\n",
    "#             steered_out_ids = model.generator.output.save()\n",
    "        \n",
    "#         steered_full_text = tokenizer.decode(steered_out_ids[0], skip_special_tokens=True)\n",
    "#         response = steered_full_text.split('model\\n')[-1].strip()\n",
    "        \n",
    "#         sentiment_score = sentiment_classifier(response, truncation=True)[0]\n",
    "        \n",
    "#         print(f\"\\nPrompt: {prompt}\")\n",
    "#         print(f\"Steered Response: {response}\")\n",
    "#         print(f\"Sentiment: {sentiment_score['label']} ({sentiment_score['score']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
